# Personal Assistant

**A transparent, hackable AI assistant that evolves with you.**

## Philosophy

This project prioritizes **transparency over polish**. The entire system is ~1000 lines of Python and a simple React frontend—no magic, no hidden complexity. You can read every line of code that touches files, executes commands, or stores data.

You're not just an AI running inside this codebase. You *are* the assistant, and you can modify yourself: fix bugs, add features, improve skills, even rewrite the wrapper application you're running inside. This is a tool that grows with its user, not one that forces adaptation to someone else's vision.

**Core principles:**
- **Developer-native**: A proper development environment, not a chatbot bolted onto a messaging app
- **Self-improving**: Teach it something once, turn it into a reusable automation
- **Local-first**: Conversations, memory, and credentials stay on the user's machine

## What You Can Do

- **Chat** through a web interface with real-time streaming
- **Talk** using realtime voice mode — speak through the orchestrator via WebRTC
- **Execute** code, manage files, run shell commands—full Claude Code capabilities
- **Remember** context across sessions with searchable conversation history
- **Automate** workflows through custom skills (slash commands)
- **Evolve** by creating new skills and modifying your own behavior

---

## Project Structure

This is a self-contained Claude Code environment. Everything runs within this folder: the wrapper application (api/manager/frontend), skills, agents, scripts, and memories.

```
assistant/
├── context/            # Private data submodule (assistant-context repo)
│   ├── *.jsonl         # Conversation JSONL files
│   ├── <uuid>/         # SDK state directories (subagents, tool-results)
│   ├── memory/         # Memory markdown files
│   ├── skills/         # ALL skill definitions (personal + general)
│   ├── scripts/        # ALL scripts (personal + general)
│   ├── secrets/        # OAuth credentials and tokens
│   ├── certs/          # SSL certificates
│   └── .env            # Environment variables
├── skills/ → context/skills/    # Symlink for backward compatibility
├── scripts/ → context/scripts/  # Symlink for backward compatibility
├── default-skills/     # Symlinks to general-purpose skills (shareable)
├── default-scripts/    # Symlinks to general-purpose scripts (shareable)
├── .claude_config/     # Claude Code SDK config (symlinks to context/)
├── index/              # Vector search index (gitignored)
├── utils/              # Shared Python utilities (paths.py)
├── agents/             # Agent definitions
├── manager/            # Python wrapper for Claude Code SDK (~400 lines)
├── orchestrator/       # Orchestrator agent — controls multiple Claude Code instances
│   └── providers/      # Model providers (Anthropic text, OpenAI realtime voice)
├── api/                # FastAPI server (REST + WebSocket, ~300 lines)
├── frontend/           # React multi-tab chat interface
├── tests/              # Test suite
└── .venv/              # Python virtual environment
```

`CLAUDE_CONFIG_DIR` is set to `.claude_config/` by `scripts/run.sh`. A symlink at `.claude_config/projects/<mangled-path>` points to `context/` for SDK compatibility. All internal code references `context/` directly via `utils/paths.py`.

---

## Reference

### The Wrapper Application

The wrapper (api + manager + orchestrator + frontend) provides a multi-tab web interface for interacting with Claude Code. The orchestrator agent can control multiple Claude Code instances simultaneously and supports both text and realtime voice modes. When running inside the wrapper, you can edit its own code—the manager, API routes, frontend components—and those changes affect the very application you're running in.

**Session IDs:** Each session has a stable `local_id` (UUID, generated by frontend, never changes) and an `sdk_session_id` (from Claude SDK, used for resume/JSONL). The `local_id` is the primary key for the session pool, tabs, and orchestrator.

Start the backend: `scripts/run.sh -m uvicorn api.app:create_app --factory --port 8000`

Start the frontend: `cd frontend && npm run dev`

Or use `/debug-app` which handles both and provides browser automation.

### Memory & History

Context files live at `context/` — a Git submodule (`assistant-context` repo):

```
context/
├── *.jsonl            # Conversation history (JSONL files)
├── .titles.json       # Custom session titles
├── <uuid>/            # SDK state dirs (subagents, tool-results)
├── memory/            # Memory files (Markdown)
│   ├── MEMORY.md      # Index file (keep under 200 lines)
│   └── *.md           # Detailed documents
├── skills/            # All skill definitions
├── scripts/           # All scripts
├── secrets/           # OAuth credentials and tokens
├── certs/             # SSL certificates
└── .env               # Environment variables
```

**How to use memory properly:**
- `MEMORY.md` is the index file—keep it under 200 lines with references only
- Store detailed plans, decisions, and context in **separate files** (e.g., `some-plan.md`)
- In `MEMORY.md`, add one-line references: `- filename.md — Brief description`
- This keeps the index scannable while allowing unlimited detail in linked files

Both memory and conversation history are indexed automatically for semantic search via `/recall <query>`:
- **Memory**: Indexed immediately when files change (file watcher)
- **History**: Indexed every 2 minutes (if files changed)

**Note**: A symlink at `.claude_config/projects/<mangled>/` points to `context/` for Claude SDK compatibility. Our code references `context/` directly via `utils/paths.py`—never the symlink.

### Voice Mode (Realtime)

The orchestrator supports a realtime voice mode powered by the OpenAI Realtime API via WebRTC. Audio flows directly between the browser and OpenAI for low latency; the backend only handles signaling, tool execution, and persistence.

**Architecture:**
- Frontend establishes a WebRTC connection to OpenAI using an ephemeral token from the backend (`POST /api/orchestrator/voice/session`)
- Audio streams directly between browser ↔ OpenAI (sub-100ms latency)
- The frontend mirrors all OpenAI data channel events to the backend via the orchestrator WebSocket (`voice_event` messages)
- The backend processes tool calls and sends commands back (`voice_command` messages) for the frontend to forward to OpenAI
- Server-side VAD (voice activity detection) — no push-to-talk needed

**Key files:**
- `api/routes/voice.py` — Ephemeral token endpoint (exchanges `OPENAI_API_KEY` for a short-lived token)
- `orchestrator/providers/openai_voice.py` — `OpenAIVoiceProvider` that translates OpenAI Realtime events into `OrchestratorEvent`s
- `orchestrator/session.py` — Voice session lifecycle, tool execution, JSONL persistence
- `frontend/src/hooks/useVoiceSession.ts` — WebRTC connection management (SDP exchange, mic, data channel)
- `frontend/src/hooks/useVoiceOrchestrator.ts` — Bridges the WebRTC session and orchestrator WebSocket
- `frontend/src/components/VoiceButton.tsx` — Mic toggle UI with states: off, connecting, active, speaking, thinking, tool_use, error
- `frontend/src/api/voice.ts` — API client for ephemeral token and SDP exchange

**Environment:** Requires `OPENAI_API_KEY` set in the environment. Model: `gpt-realtime`, voice: `cedar`.

**Tool sharing:** Both text (Anthropic) and voice (OpenAI) modes use the same `ToolRegistry`. `get_definitions()` returns Anthropic format; `get_openai_definitions()` returns OpenAI function-calling format.

**JSONL persistence:** Voice turns are saved with `"source": "voice_transcription"` / `"voice_response"` fields. User transcripts are prefixed with `[voice]`. Interruptions are logged as `voice_interrupted` entries.

### Self-Modification

You can extend and modify your own capabilities:

- **Skills** (`skills/`): Create with `/scaffold-skill`, modify existing ones directly
- **Agents** (`agents/`): Create with `/scaffold-agent` for specialized subagents
- **Scripts** (`scripts/`): Shared tools any skill can reference
- **Wrapper** (`api/`, `manager/`, `frontend/`): The application code itself

Run Python scripts through the venv: `scripts/run.sh scripts/<script>.py [args]`

### Skill and Script Maintenance

You have an active role in maintaining and improving skills and scripts. When you identify any problems, gaps, or issues with a skill or script, you should:

1. Think about how to address and fix that issue.
2. Present options to the user for addressing and fixing the problem.
3. Offer to spin up a nested agent to work on that specific skill or script improvement.

This ensures that the assistant is always evolving and that skills/scripts remain up-to-date and effective.

### Writing Skills

- Format: YAML frontmatter + markdown instructions
- Never use literal backtick command syntax in SKILL.md (triggers permission prompts)
- Variable substitution: `$ARGUMENTS`, `$0`/`$1`/`$2`, `${CLAUDE_SESSION_ID}`

### Testing

Run the full suite: `scripts/run.sh -m pytest tests/ -v`

Write tests alongside code. Mock external dependencies with `unittest.mock`.

### Browser Automation

Chrome DevTools MCP provides full browser control. Use `/debug-app` for integrated frontend testing.

### Compact Instructions

When compacting, preserve:
- Current task context and progress
- Key decisions made during this session
- Key learnings from this session
- Any unresolved questions or blockers
